{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langcahianライブラリのインストール\n",
    "!pip install langchain langchain-community\n",
    "\n",
    "# openaiライブラリのインストール\n",
    "!pip install openai\n",
    "\n",
    "# lnagchainでopenaiを使うためのライブラリをインストール\n",
    "!pip install langchain_openai\n",
    "\n",
    " #pdfを読み込むためのライブラリをインストールする\n",
    "!pip install pypdf\n",
    "\n",
    "# embeddingで利用するライブラリのインストール\n",
    "!pip install tiktoken\n",
    "\n",
    "# ローカルマシン上で実行できるchromaベクターデータベースのインストール\n",
    "!pip install chromadb\n",
    "\n",
    "# 簡単なGUIを作成できるライブラリのインストール\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "#APIキーの登録\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"__YOUR KEY__\"\n",
    "\n",
    "url = \"https://api.openai.iniad.org/api/v1/\"\n",
    "\n",
    "# OpenAI埋め込みモデルのインスタンスを作成\n",
    "embeddings_model = OpenAIEmbeddings(\n",
    "    openai_api_base= url\n",
    ")\n",
    "\n",
    "\n",
    "# PDFファイルの読み込み\n",
    "loader = PyPDFLoader(\"Disease.pdf\")\n",
    "text = \"\\n\".join([page.page_content for page in loader.load()])\n",
    "\n",
    "# Text Splitterの設定\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    add_start_index=True\n",
    ")\n",
    "\n",
    "# テキストの分割\n",
    "documents = text_splitter.create_documents([text])\n",
    "\n",
    "# ベクトル化したテキストをChromaDBに保存する\n",
    "db = Chroma.from_documents(documents,embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"インフルエンザの症状を教えてください\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector = OpenAIEmbeddings(openai_api_base= url).embed_query(query)\n",
    "docs = db.similarity_search_by_vector(embedding_vector)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリをインポートする\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# retriever(検索対象のVectorDB)の定義\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# テンプレートを定義\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "# promptを定義。これがLLMの入力になる\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# LLMを定義。今回はChatGPTの4o-miniを利用する\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    openai_api_base= url,\n",
    "    verbose=True\n",
    "    )\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "# Chainを定義。これはおまじないだと思って下さい。\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser(verbose=True)\n",
    "    )\n",
    "\n",
    "# 問い合わせしたい内容を定義\n",
    "input = \"インフルエンザの症状を教えてください\"\n",
    "\n",
    "references = retriever.get_relevant_documents(input)\n",
    "output_by_retriever = chain.invoke(input)\n",
    "output_by_simple_llm = llm.invoke(input)\n",
    "\n",
    "# 参考にした箇所を表示します。\n",
    "# 情報の取得元や参照した文献など、生成された回答の背景にある情報源が含まれています\n",
    "print(\"参考にした箇所: \" + str(references))\n",
    "\n",
    "\n",
    "# 検索結果を基に生成された回答や内容が含まれています\n",
    "print(\"RAGの出力結果: \" + str(output_by_retriever))\n",
    "\n",
    "# シンプルな言語モデルのみを使用して生成された回答や内容を含んでいます\n",
    "print(\"シンプルなLLM（ChatGPT）の出力結果: \" + str(output_by_simple_llm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# ドキュメントの読み込みとベクトルストアの作成\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# テンプレートとプロンプトの定義\n",
    "template_with_context = \"\"\"以下のcontextのみに基づいて質問にできるだけ詳しく箇条書きで答えなさい。:\n",
    "{context}\n",
    "質問: {question}\n",
    "\"\"\"\n",
    "template_without_context = \"\"\"質問にできるだけ詳しく箇条書きで答えなさい。:\n",
    "質問: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_with_context = ChatPromptTemplate.from_template(template_with_context)\n",
    "prompt_without_context = ChatPromptTemplate.from_template(template_without_context)\n",
    "\n",
    "# LLMの定義（ストリーミング対応）\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    openai_api_base= url,\n",
    "    verbose=True)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "# Chainの定義\n",
    "chain_with_context = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt_with_context\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain_without_context = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | prompt_without_context\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "def process_query_with_rag(query):\n",
    "    # 関連ドキュメントの取得\n",
    "    relevant_docs = retriever.get_relevant_documents(query)\n",
    "    sources = [doc.page_content for doc in relevant_docs]\n",
    "\n",
    "    # 回答の生成（ストリーミング）\n",
    "    answer = \"\"\n",
    "    for chunk in chain_with_context.stream(query):\n",
    "        answer += chunk\n",
    "        yield answer, \"\\n\\n\".join(sources)\n",
    "\n",
    "def process_query_without_rag(query):\n",
    "    # 回答の生成（ストリーミング）\n",
    "    answer = \"\"\n",
    "    for chunk in chain_without_context.stream(query):\n",
    "        answer += chunk\n",
    "        yield answer, \"なし\"\n",
    "\n",
    "# Gradioインターフェースの定義\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"## RAGあり\")\n",
    "            input_text_with_rag = gr.Textbox(lines=2, placeholder=\"質問を入力してください...\")\n",
    "            output_text_with_rag = gr.Markdown(label=\"回答\")\n",
    "            sources_with_rag = gr.Textbox(label=\"参照ソース\")\n",
    "            rag_button = gr.Button(\"RAGありで質問する\")\n",
    "            rag_button.click(fn=process_query_with_rag, inputs=input_text_with_rag, outputs=[output_text_with_rag, sources_with_rag])\n",
    "\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"## RAGなし\")\n",
    "            input_text_without_rag = gr.Textbox(lines=2, placeholder=\"質問を入力してください...\")\n",
    "            output_text_without_rag = gr.Markdown(label=\"回答\")\n",
    "            sources_without_rag = gr.Textbox(label=\"参照ソース\")\n",
    "            no_rag_button = gr.Button(\"RAGなしで質問する\")\n",
    "            no_rag_button.click(fn=process_query_without_rag, inputs=input_text_without_rag, outputs=[output_text_without_rag, sources_without_rag])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
